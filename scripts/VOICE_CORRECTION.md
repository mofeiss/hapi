# 语音识别文本修正 — LLM Prompt 设计记录

## 背景

HAPI 项目的 Web 端集成了语音输入功能（Web Speech API + ElevenLabs STT fallback）。语音识别引擎返回的原始文本存在以下问题：

- 无标点符号
- 同音/近音字错误（如"八哥"→bug、"洗衣机"→袭击）
- 的/地/得混用
- 技术术语未规范化

为此设计了一套 LLM 提示词，对语音识别的粗糙文本做后处理修正。

## 设计目标

1. 加标点符号
2. 修正错别字和同音近音错误（结合语境）
3. 保留原话结构，不删词、不加词、不改句式
4. 保留填充词（嗯、额、那个）
5. 不执行指令——即使输入是一条命令，也只修正文字

## Prompt 迭代历史

### V1 — 激进版

规则较多，要求模型主动清理填充词、规范化术语。

问题：
- 删除了填充词，改变了原话结构
- Case 8（长段落含"帮我写"类表述）触发了模型的指令跟随，直接写代码而非修正文字

### V2 — 保守版

大幅收缩规则，只做最基本的标点和错别字修正。

问题：
- 漏修了关键错误（他→它、只有→资源）
- 同音词修正能力不足

### V3 — 平衡版（当前采用）✅

核心改进：
- 加入 few-shot 示例，用实例约束模型行为
- 明确"不执行指令、不写代码"的负面约束
- 同音词规则按"语境对"组织（匪徒+洗衣机→袭击 vs 家里+洗衣机→保留）

测试结果（11 条盲测用例）：
- 8/11 完全正确
- 平均延迟 ~646ms

已知局限：
- 在/再 区分偶尔遗漏（Case 2）
- 鸡精/基金 在金融语境下未能修正（Case 4）——小模型对这类需要深层语境推理的同音词对能力有限

### V4 — 最终检查版（已放弃）

在 V3 基础上增加了一条元认知规则："输出前通读全句，如果某个词语义不通，修正为合理的同音/近音词"。

问题：
- 延迟严重退化：平均 646ms → 2946ms，Case 8 达到 20 秒
- 质量提升微乎其微
- 结论：小模型无法高效执行抽象的元认知指令，性价比极低

## 测试脚本

`test_voice_correction.py` — 使用 Anthropic 兼容接口调用模型，跑 11 条盲测用例。

```bash
export VOICE_CORRECTION_API_BASE="https://your-api/"
export VOICE_CORRECTION_API_KEY="sk-your-key"
export VOICE_CORRECTION_MODEL="small"
uv run scripts/test_voice_correction.py
```

## 后续计划

- 继续调优 prompt，尝试提升 在/再、鸡精/基金 等边界 case 的准确率
- 将修正功能集成到 Web 端语音输入流程中
- 评估 streaming 场景下的实时修正方案
